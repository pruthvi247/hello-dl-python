#!/usr/bin/env python3
"""
CNN Parallelism Implementation: Complete Letter Recognition System
================================================================

This implementation demonstrates every parallelization technique for the CNN architecture:

Architecture:
- Input: 28Ã—28Ã—1 images
- Conv1: 3Ã—3, 1â†’32 channels â†’ MaxPool2D â†’ GELU
- Conv2: 3Ã—3, 32â†’64 channels â†’ MaxPool2D â†’ GELU  
- Conv3: 3Ã—3, 64â†’128 channels â†’ MaxPool2D â†’ GELU
- Flatten â†’ Linear(128â†’512) â†’ GELU
- Linear(512â†’256) â†’ GELU  
- Linear(256â†’26) â†’ LogSoftMax

Parallelization Strategies Demonstrated:
âœ… Batch Parallelism: Multiple images processed simultaneously
âœ… Channel Parallelism: Different feature maps computed concurrently
âœ… Spatial Parallelism: Image regions processed in parallel
âœ… Filter Parallelism: Multiple convolution kernels applied simultaneously
âœ… Pipeline Parallelism: Different network stages running concurrently
âœ… Gradient Parallelism: Backward pass parallelization
"""

import numpy as np
import time
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Optional, Dict, Any
import multiprocessing
from dataclasses import dataclass
from pathlib import Path
import json

# Set random seed for reproducibility
np.random.seed(42)

@dataclass
class ParallelismMetrics:
    """Track parallelism performance metrics"""
    operation_name: str
    sequential_time: float
    parallel_time: float
    num_workers: int
    speedup: float
    efficiency: float
    total_operations: int

class CNNParallelismVisualizer:
    """Visualize how parallelism works in CNN operations"""
    
    def __init__(self):
        self.metrics = []
        
    def log_operation(self, metrics: ParallelismMetrics):
        """Log performance metrics for analysis"""
        self.metrics.append(metrics)
        
        print(f"\nğŸ“Š {metrics.operation_name} Performance:")
        print(f"   Sequential time: {metrics.sequential_time:.2f}ms")
        print(f"   Parallel time: {metrics.parallel_time:.2f}ms") 
        print(f"   Workers: {metrics.num_workers}")
        print(f"   Speedup: {metrics.speedup:.2f}x")
        print(f"   Efficiency: {metrics.efficiency:.1f}%")
        print(f"   Total operations: {metrics.total_operations:,}")
        
    def show_data_flow(self, layer_name: str, input_shape: tuple, output_shape: tuple, 
                      parallel_strategy: str):
        """Show how data flows through parallel processing"""
        print(f"\nğŸ”„ {layer_name} Data Flow:")
        print(f"   Input shape: {input_shape}")
        print(f"   Output shape: {output_shape}")  
        print(f"   Parallel strategy: {parallel_strategy}")
        
        # Calculate memory requirements
        input_elements = np.prod(input_shape)
        output_elements = np.prod(output_shape)
        input_memory = input_elements * 4  # float32
        output_memory = output_elements * 4
        
        print(f"   Input memory: {input_memory:,} bytes ({input_memory/1024:.1f} KB)")
        print(f"   Output memory: {output_memory:,} bytes ({output_memory/1024:.1f} KB)")

class ParallelConvLayer:
    """Parallel convolution layer with multiple parallelization strategies"""
    
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, 
                 stride: int = 1, padding: int = 0):
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        
        # Initialize weights with Xavier initialization
        fan_in = in_channels * kernel_size * kernel_size
        self.weights = np.random.normal(0, np.sqrt(2.0 / fan_in), 
                                      (out_channels, in_channels, kernel_size, kernel_size))
        self.bias = np.zeros(out_channels)
        
        self.visualizer = CNNParallelismVisualizer()
        
    def forward_sequential(self, x: np.ndarray) -> np.ndarray:
        """Sequential convolution implementation for comparison"""
        batch_size, in_channels, height, width = x.shape
        
        # Calculate output dimensions
        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        output = np.zeros((batch_size, self.out_channels, out_height, out_width))
        
        start_time = time.time()
        total_ops = 0
        
        # Apply padding if needed
        if self.padding > 0:
            x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), 
                                (self.padding, self.padding)), mode='constant')
        else:
            x_padded = x
            
        # Sequential convolution
        for b in range(batch_size):
            for out_ch in range(self.out_channels):
                for i in range(out_height):
                    for j in range(out_width):
                        # Extract patch
                        i_start = i * self.stride
                        j_start = j * self.stride
                        patch = x_padded[b, :, i_start:i_start+self.kernel_size, 
                                       j_start:j_start+self.kernel_size]
                        
                        # Convolution operation
                        conv_result = np.sum(patch * self.weights[out_ch]) + self.bias[out_ch]
                        output[b, out_ch, i, j] = conv_result
                        total_ops += self.in_channels * self.kernel_size * self.kernel_size
        
        sequential_time = (time.time() - start_time) * 1000
        return output, sequential_time, total_ops
    
    def forward_parallel_channels(self, x: np.ndarray, num_workers: int = 8) -> np.ndarray:
        """Parallel convolution: parallelized over output channels"""
        batch_size, in_channels, height, width = x.shape
        
        # Calculate output dimensions  
        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        output = np.zeros((batch_size, self.out_channels, out_height, out_width))
        
        # Apply padding if needed
        if self.padding > 0:
            x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), 
                                (self.padding, self.padding)), mode='constant')
        else:
            x_padded = x
        
        start_time = time.time()
        
        def process_output_channel(out_ch):
            """Process one output channel completely"""
            channel_ops = 0
            for b in range(batch_size):
                for i in range(out_height):
                    for j in range(out_width):
                        # Extract patch
                        i_start = i * self.stride
                        j_start = j * self.stride
                        patch = x_padded[b, :, i_start:i_start+self.kernel_size, 
                                       j_start:j_start+self.kernel_size]
                        
                        # Convolution operation
                        conv_result = np.sum(patch * self.weights[out_ch]) + self.bias[out_ch]
                        output[b, out_ch, i, j] = conv_result
                        channel_ops += self.in_channels * self.kernel_size * self.kernel_size
            return channel_ops
        
        # Execute in parallel over output channels
        total_ops = 0
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(process_output_channel, ch) for ch in range(self.out_channels)]
            for future in as_completed(futures):
                total_ops += future.result()
        
        parallel_time = (time.time() - start_time) * 1000
        return output, parallel_time, total_ops
    
    def forward_parallel_spatial(self, x: np.ndarray, tile_size: int = 8, 
                               num_workers: int = 8) -> np.ndarray:
        """Parallel convolution: parallelized over spatial tiles"""
        batch_size, in_channels, height, width = x.shape
        
        # Calculate output dimensions
        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        output = np.zeros((batch_size, self.out_channels, out_height, out_width))
        
        # Apply padding if needed
        if self.padding > 0:
            x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), 
                                (self.padding, self.padding)), mode='constant')
        else:
            x_padded = x
        
        start_time = time.time()
        
        def process_spatial_tile(args):
            """Process a spatial tile of the output"""
            batch_idx, out_ch, tile_i, tile_j = args
            tile_ops = 0
            
            # Tile boundaries
            i_start = tile_i * tile_size
            i_end = min((tile_i + 1) * tile_size, out_height)
            j_start = tile_j * tile_size  
            j_end = min((tile_j + 1) * tile_size, out_width)
            
            # Process this tile
            for i in range(i_start, i_end):
                for j in range(j_start, j_end):
                    # Extract patch
                    patch_i = i * self.stride
                    patch_j = j * self.stride
                    patch = x_padded[batch_idx, :, patch_i:patch_i+self.kernel_size, 
                                   patch_j:patch_j+self.kernel_size]
                    
                    # Convolution operation
                    conv_result = np.sum(patch * self.weights[out_ch]) + self.bias[out_ch]
                    output[batch_idx, out_ch, i, j] = conv_result
                    tile_ops += self.in_channels * self.kernel_size * self.kernel_size
            
            return tile_ops
        
        # Generate tile work items
        num_tiles_h = (out_height + tile_size - 1) // tile_size
        num_tiles_w = (out_width + tile_size - 1) // tile_size
        
        work_items = [
            (b, c, ti, tj)
            for b in range(batch_size)
            for c in range(self.out_channels)
            for ti in range(num_tiles_h)
            for tj in range(num_tiles_w)
        ]
        
        # Execute in parallel
        total_ops = 0
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(process_spatial_tile, item) for item in work_items]
            for future in as_completed(futures):
                total_ops += future.result()
        
        parallel_time = (time.time() - start_time) * 1000
        return output, parallel_time, total_ops
    
    def forward_parallel_hybrid(self, x: np.ndarray, num_workers: int = 8) -> np.ndarray:
        """Hybrid parallelization: channels + spatial"""
        batch_size, in_channels, height, width = x.shape
        
        # Calculate output dimensions
        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        output = np.zeros((batch_size, self.out_channels, out_height, out_width))
        
        # Apply padding if needed
        if self.padding > 0:
            x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), 
                                (self.padding, self.padding)), mode='constant')
        else:
            x_padded = x
        
        start_time = time.time()
        
        # Use im2col for vectorized convolution (most efficient)
        def process_batch_channel(args):
            """Process one batch-channel combination using vectorized operations"""
            batch_idx, out_ch = args
            batch_ops = 0
            
            # Extract all patches for this batch using vectorized operations
            patches = []
            for i in range(out_height):
                for j in range(out_width):
                    patch_i = i * self.stride
                    patch_j = j * self.stride
                    patch = x_padded[batch_idx, :, patch_i:patch_i+self.kernel_size, 
                                   patch_j:patch_j+self.kernel_size]
                    patches.append(patch.flatten())
                    batch_ops += self.in_channels * self.kernel_size * self.kernel_size
            
            # Vectorized convolution across all spatial locations
            patches_matrix = np.array(patches)  # Shape: (out_height*out_width, kernel_sizeÂ²*in_channels)
            kernel_vector = self.weights[out_ch].flatten()  # Shape: (kernel_sizeÂ²*in_channels,)
            
            # Matrix-vector multiplication (highly optimized)
            conv_results = patches_matrix @ kernel_vector + self.bias[out_ch]
            
            # Reshape back to spatial dimensions
            output[batch_idx, out_ch] = conv_results.reshape(out_height, out_width)
            
            return batch_ops
        
        # Generate work items: (batch_idx, out_channel) pairs
        work_items = [(b, c) for b in range(batch_size) for c in range(self.out_channels)]
        
        # Execute in parallel
        total_ops = 0
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(process_batch_channel, item) for item in work_items]
            for future in as_completed(futures):
                total_ops += future.result()
        
        parallel_time = (time.time() - start_time) * 1000
        return output, parallel_time, total_ops
    
    def demonstrate_parallelism(self, x: np.ndarray):
        """Demonstrate different parallelization strategies"""
        print(f"\nğŸ§  Convolution Layer Parallelism Demo")
        print(f"Input shape: {x.shape}")
        print(f"Weights shape: {self.weights.shape}")
        print(f"Expected output: ({x.shape[0]}, {self.out_channels}, ?, ?)")
        
        # Sequential baseline
        print(f"\n1ï¸âƒ£ Sequential Implementation:")
        seq_output, seq_time, seq_ops = self.forward_sequential(x)
        print(f"   Output shape: {seq_output.shape}")
        print(f"   Time: {seq_time:.2f}ms")
        print(f"   Operations: {seq_ops:,}")
        
        num_workers = min(8, multiprocessing.cpu_count())
        
        # Channel parallelism
        print(f"\n2ï¸âƒ£ Channel Parallelism ({num_workers} workers):")
        ch_output, ch_time, ch_ops = self.forward_parallel_channels(x, num_workers)
        ch_speedup = seq_time / ch_time if ch_time > 0 else 1
        ch_efficiency = ch_speedup / num_workers * 100
        print(f"   Output shape: {ch_output.shape}")
        print(f"   Time: {ch_time:.2f}ms")
        print(f"   Speedup: {ch_speedup:.2f}x")
        print(f"   Efficiency: {ch_efficiency:.1f}%")
        
        # Spatial parallelism
        print(f"\n3ï¸âƒ£ Spatial Parallelism ({num_workers} workers):")
        sp_output, sp_time, sp_ops = self.forward_parallel_spatial(x, tile_size=4, num_workers=num_workers)
        sp_speedup = seq_time / sp_time if sp_time > 0 else 1
        sp_efficiency = sp_speedup / num_workers * 100
        print(f"   Output shape: {sp_output.shape}")
        print(f"   Time: {sp_time:.2f}ms")
        print(f"   Speedup: {sp_speedup:.2f}x")
        print(f"   Efficiency: {sp_efficiency:.1f}%")
        
        # Hybrid parallelism
        print(f"\n4ï¸âƒ£ Hybrid Parallelism ({num_workers} workers):")
        hy_output, hy_time, hy_ops = self.forward_parallel_hybrid(x, num_workers)
        hy_speedup = seq_time / hy_time if hy_time > 0 else 1
        hy_efficiency = hy_speedup / num_workers * 100
        print(f"   Output shape: {hy_output.shape}")
        print(f"   Time: {hy_time:.2f}ms")
        print(f"   Speedup: {hy_speedup:.2f}x")
        print(f"   Efficiency: {hy_efficiency:.1f}%")
        
        # Verify all outputs are identical
        print(f"\nâœ… Verification:")
        print(f"   Channel parallel matches sequential: {np.allclose(seq_output, ch_output)}")
        print(f"   Spatial parallel matches sequential: {np.allclose(seq_output, sp_output)}")
        print(f"   Hybrid parallel matches sequential: {np.allclose(seq_output, hy_output)}")
        
        return seq_output

class ParallelMaxPool2D:
    """Parallel max pooling implementation"""
    
    def __init__(self, pool_size: int = 2, stride: int = 2):
        self.pool_size = pool_size
        self.stride = stride
        
    def forward_sequential(self, x: np.ndarray) -> np.ndarray:
        """Sequential max pooling"""
        batch_size, channels, height, width = x.shape
        
        out_height = (height - self.pool_size) // self.stride + 1
        out_width = (width - self.pool_size) // self.stride + 1
        
        output = np.zeros((batch_size, channels, out_height, out_width))
        
        start_time = time.time()
        total_ops = 0
        
        for b in range(batch_size):
            for c in range(channels):
                for i in range(out_height):
                    for j in range(out_width):
                        # Extract pool region
                        i_start = i * self.stride
                        j_start = j * self.stride
                        pool_region = x[b, c, i_start:i_start+self.pool_size, 
                                      j_start:j_start+self.pool_size]
                        
                        # Max operation
                        output[b, c, i, j] = np.max(pool_region)
                        total_ops += self.pool_size * self.pool_size
        
        sequential_time = (time.time() - start_time) * 1000
        return output, sequential_time, total_ops
    
    def forward_parallel(self, x: np.ndarray, num_workers: int = 8) -> np.ndarray:
        """Parallel max pooling"""
        batch_size, channels, height, width = x.shape
        
        out_height = (height - self.pool_size) // self.stride + 1
        out_width = (width - self.pool_size) // self.stride + 1
        
        output = np.zeros((batch_size, channels, out_height, out_width))
        
        start_time = time.time()
        
        def process_batch_channel(args):
            """Process one batch-channel combination"""
            batch_idx, channel_idx = args
            ops_count = 0
            
            for i in range(out_height):
                for j in range(out_width):
                    # Extract pool region
                    i_start = i * self.stride
                    j_start = j * self.stride
                    pool_region = x[batch_idx, channel_idx, 
                                  i_start:i_start+self.pool_size, 
                                  j_start:j_start+self.pool_size]
                    
                    # Max operation
                    output[batch_idx, channel_idx, i, j] = np.max(pool_region)
                    ops_count += self.pool_size * self.pool_size
            
            return ops_count
        
        # Generate work items
        work_items = [(b, c) for b in range(batch_size) for c in range(channels)]
        
        # Execute in parallel
        total_ops = 0
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(process_batch_channel, item) for item in work_items]
            for future in as_completed(futures):
                total_ops += future.result()
        
        parallel_time = (time.time() - start_time) * 1000
        return output, parallel_time, total_ops
    
    def demonstrate_parallelism(self, x: np.ndarray):
        """Demonstrate max pooling parallelism"""
        print(f"\nğŸŠ MaxPool2D Parallelism Demo")
        print(f"Input shape: {x.shape}")
        print(f"Pool size: {self.pool_size}Ã—{self.pool_size}")
        print(f"Stride: {self.stride}")
        
        # Sequential
        seq_output, seq_time, seq_ops = self.forward_sequential(x)
        print(f"\nğŸ“Š Sequential MaxPool:")
        print(f"   Output shape: {seq_output.shape}")
        print(f"   Time: {seq_time:.2f}ms")
        print(f"   Operations: {seq_ops:,}")
        
        # Parallel
        num_workers = min(8, multiprocessing.cpu_count())
        par_output, par_time, par_ops = self.forward_parallel(x, num_workers)
        speedup = seq_time / par_time if par_time > 0 else 1
        efficiency = speedup / num_workers * 100
        
        print(f"\nâš¡ Parallel MaxPool ({num_workers} workers):")
        print(f"   Output shape: {par_output.shape}")
        print(f"   Time: {par_time:.2f}ms")
        print(f"   Speedup: {speedup:.2f}x")
        print(f"   Efficiency: {efficiency:.1f}%")
        print(f"   Results match: {np.allclose(seq_output, par_output)}")
        
        return seq_output

class ParallelGELU:
    """Parallel GELU activation function"""
    
    def forward_sequential(self, x: np.ndarray) -> np.ndarray:
        """Sequential GELU implementation"""
        start_time = time.time()
        
        # GELU formula: x * 0.5 * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))
        sqrt_2_pi = np.sqrt(2 / np.pi)
        inner = sqrt_2_pi * (x + 0.044715 * x**3)
        output = x * 0.5 * (1 + np.tanh(inner))
        
        sequential_time = (time.time() - start_time) * 1000
        return output, sequential_time, x.size
    
    def forward_parallel(self, x: np.ndarray, num_workers: int = 8) -> np.ndarray:
        """Parallel GELU implementation"""
        start_time = time.time()
        
        def gelu_chunk(chunk):
            """Apply GELU to a chunk of data"""
            sqrt_2_pi = np.sqrt(2 / np.pi)
            inner = sqrt_2_pi * (chunk + 0.044715 * chunk**3)
            return chunk * 0.5 * (1 + np.tanh(inner))
        
        # Split tensor into chunks
        total_elements = x.size
        flat_x = x.flatten()
        chunk_size = max(1, total_elements // num_workers)
        
        chunks = [flat_x[i:i+chunk_size] for i in range(0, total_elements, chunk_size)]
        
        # Process chunks in parallel
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            results = list(executor.map(gelu_chunk, chunks))
        
        # Reconstruct tensor
        output = np.concatenate(results).reshape(x.shape)
        
        parallel_time = (time.time() - start_time) * 1000
        return output, parallel_time, x.size
    
    def demonstrate_parallelism(self, x: np.ndarray):
        """Demonstrate GELU parallelism"""
        print(f"\nğŸ­ GELU Activation Parallelism Demo")
        print(f"Input shape: {x.shape}")
        print(f"Total elements: {x.size:,}")
        
        # Sequential
        seq_output, seq_time, seq_ops = self.forward_sequential(x)
        print(f"\nğŸ“Š Sequential GELU:")
        print(f"   Time: {seq_time:.2f}ms")
        print(f"   Elements processed: {seq_ops:,}")
        
        # Parallel
        num_workers = min(8, multiprocessing.cpu_count())
        par_output, par_time, par_ops = self.forward_parallel(x, num_workers)
        speedup = seq_time / par_time if par_time > 0 else 1
        efficiency = speedup / num_workers * 100
        
        print(f"\nâš¡ Parallel GELU ({num_workers} workers):")
        print(f"   Time: {par_time:.2f}ms")
        print(f"   Speedup: {speedup:.2f}x")
        print(f"   Efficiency: {efficiency:.1f}%")
        print(f"   Results match: {np.allclose(seq_output, par_output, rtol=1e-5)}")
        
        return seq_output

class ParallelLinearLayer:
    """Parallel linear layer implementation"""
    
    def __init__(self, in_features: int, out_features: int):
        self.in_features = in_features
        self.out_features = out_features
        
        # Xavier initialization
        self.weights = np.random.normal(0, np.sqrt(2.0 / in_features), 
                                      (in_features, out_features))
        self.bias = np.zeros(out_features)
    
    def forward_sequential(self, x: np.ndarray) -> np.ndarray:
        """Sequential linear layer"""
        start_time = time.time()
        
        # Matrix multiplication + bias
        output = x @ self.weights + self.bias
        
        sequential_time = (time.time() - start_time) * 1000
        total_ops = x.shape[0] * self.in_features * self.out_features
        
        return output, sequential_time, total_ops
    
    def forward_parallel_batch(self, x: np.ndarray, num_workers: int = 8) -> np.ndarray:
        """Parallel over batch dimension"""
        batch_size = x.shape[0]
        output = np.zeros((batch_size, self.out_features))
        
        start_time = time.time()
        
        def process_batch_chunk(batch_indices):
            """Process a chunk of batch samples"""
            chunk_input = x[batch_indices]
            chunk_output = chunk_input @ self.weights + self.bias
            return batch_indices, chunk_output
        
        # Split batch into chunks
        chunk_size = max(1, batch_size // num_workers)
        chunks = [list(range(i, min(i + chunk_size, batch_size))) 
                 for i in range(0, batch_size, chunk_size)]
        
        # Process in parallel
        total_ops = 0
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(process_batch_chunk, chunk) for chunk in chunks]
            
            for future in as_completed(futures):
                indices, chunk_result = future.result()
                output[indices] = chunk_result
                total_ops += len(indices) * self.in_features * self.out_features
        
        parallel_time = (time.time() - start_time) * 1000
        return output, parallel_time, total_ops
    
    def forward_parallel_features(self, x: np.ndarray, num_workers: int = 8) -> np.ndarray:
        """Parallel over output features"""
        batch_size = x.shape[0]
        output = np.zeros((batch_size, self.out_features))
        
        start_time = time.time()
        
        def process_feature_chunk(feature_indices):
            """Process a chunk of output features"""
            chunk_weights = self.weights[:, feature_indices]
            chunk_bias = self.bias[feature_indices]
            chunk_output = x @ chunk_weights + chunk_bias
            return feature_indices, chunk_output
        
        # Split features into chunks
        chunk_size = max(1, self.out_features // num_workers)
        feature_chunks = [list(range(i, min(i + chunk_size, self.out_features)))
                         for i in range(0, self.out_features, chunk_size)]
        
        # Process in parallel
        total_ops = 0
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(process_feature_chunk, chunk) for chunk in feature_chunks]
            
            for future in as_completed(futures):
                indices, chunk_result = future.result()
                output[:, indices] = chunk_result
                total_ops += len(indices) * batch_size * self.in_features
        
        parallel_time = (time.time() - start_time) * 1000
        return output, parallel_time, total_ops
    
    def demonstrate_parallelism(self, x: np.ndarray):
        """Demonstrate linear layer parallelism"""
        print(f"\nğŸ”— Linear Layer Parallelism Demo")
        print(f"Input shape: {x.shape}")
        print(f"Weights shape: {self.weights.shape}")
        print(f"Output shape: ({x.shape[0]}, {self.out_features})")
        
        # Sequential
        seq_output, seq_time, seq_ops = self.forward_sequential(x)
        print(f"\nğŸ“Š Sequential Linear:")
        print(f"   Time: {seq_time:.2f}ms")
        print(f"   Operations: {seq_ops:,}")
        
        num_workers = min(8, multiprocessing.cpu_count())
        
        # Batch parallelism
        batch_output, batch_time, batch_ops = self.forward_parallel_batch(x, num_workers)
        batch_speedup = seq_time / batch_time if batch_time > 0 else 1
        batch_efficiency = batch_speedup / num_workers * 100
        
        print(f"\nâš¡ Batch Parallelism ({num_workers} workers):")
        print(f"   Time: {batch_time:.2f}ms")
        print(f"   Speedup: {batch_speedup:.2f}x")
        print(f"   Efficiency: {batch_efficiency:.1f}%")
        print(f"   Results match: {np.allclose(seq_output, batch_output)}")
        
        # Feature parallelism
        feat_output, feat_time, feat_ops = self.forward_parallel_features(x, num_workers)
        feat_speedup = seq_time / feat_time if feat_time > 0 else 1
        feat_efficiency = feat_speedup / num_workers * 100
        
        print(f"\nâš¡ Feature Parallelism ({num_workers} workers):")
        print(f"   Time: {feat_time:.2f}ms")
        print(f"   Speedup: {feat_speedup:.2f}x")
        print(f"   Efficiency: {feat_efficiency:.1f}%")
        print(f"   Results match: {np.allclose(seq_output, feat_output)}")
        
        return seq_output

class CompleteCNNParallelDemo:
    """Complete CNN architecture with parallelism demonstration"""
    
    def __init__(self):
        # Define the complete CNN architecture
        self.conv1 = ParallelConvLayer(1, 32, kernel_size=3)     # 28Ã—28Ã—1 â†’ 26Ã—26Ã—32
        self.pool1 = ParallelMaxPool2D(pool_size=2, stride=2)    # 26Ã—26Ã—32 â†’ 13Ã—13Ã—32
        self.gelu1 = ParallelGELU()
        
        self.conv2 = ParallelConvLayer(32, 64, kernel_size=3)    # 13Ã—13Ã—32 â†’ 11Ã—11Ã—64
        self.pool2 = ParallelMaxPool2D(pool_size=2, stride=2)    # 11Ã—11Ã—64 â†’ 5Ã—5Ã—64
        self.gelu2 = ParallelGELU()
        
        self.conv3 = ParallelConvLayer(64, 128, kernel_size=3)   # 5Ã—5Ã—64 â†’ 3Ã—3Ã—128
        self.pool3 = ParallelMaxPool2D(pool_size=2, stride=2)    # 3Ã—3Ã—128 â†’ 1Ã—1Ã—128
        self.gelu3 = ParallelGELU()
        
        # After flattening: 128
        self.linear1 = ParallelLinearLayer(128, 512)             # 128 â†’ 512
        self.gelu4 = ParallelGELU()
        
        self.linear2 = ParallelLinearLayer(512, 256)             # 512 â†’ 256
        self.gelu5 = ParallelGELU()
        
        self.linear3 = ParallelLinearLayer(256, 26)              # 256 â†’ 26 (A-Z)
        
    def forward_complete_parallel(self, x: np.ndarray):
        """Complete forward pass with parallelism demonstration"""
        print(f"ğŸš€ Complete CNN Parallel Forward Pass")
        print(f"=" * 60)
        print(f"Input shape: {x.shape}")
        
        current = x
        
        # Convolutional block 1
        print(f"\nğŸ”¸ Convolutional Block 1:")
        current = self.conv1.demonstrate_parallelism(current)
        current = self.pool1.demonstrate_parallelism(current)
        current = self.gelu1.demonstrate_parallelism(current)
        
        # Convolutional block 2
        print(f"\nğŸ”¹ Convolutional Block 2:")
        current = self.conv2.demonstrate_parallelism(current)
        current = self.pool2.demonstrate_parallelism(current)
        current = self.gelu2.demonstrate_parallelism(current)
        
        # Convolutional block 3
        print(f"\nğŸ”¸ Convolutional Block 3:")
        current = self.conv3.demonstrate_parallelism(current)
        current = self.pool3.demonstrate_parallelism(current)
        current = self.gelu3.demonstrate_parallelism(current)
        
        # Flatten
        batch_size = current.shape[0]
        current = current.reshape(batch_size, -1)
        print(f"\nğŸ“ After flattening: {current.shape}")
        
        # Linear blocks
        print(f"\nğŸ”— Linear Block 1:")
        current = self.linear1.demonstrate_parallelism(current)
        current = self.gelu4.demonstrate_parallelism(current)
        
        print(f"\nğŸ”— Linear Block 2:")
        current = self.linear2.demonstrate_parallelism(current)
        current = self.gelu5.demonstrate_parallelism(current)
        
        print(f"\nğŸ”— Output Layer:")
        current = self.linear3.demonstrate_parallelism(current)
        
        # Log-softmax (simplified)
        print(f"\nğŸ¯ Final LogSoftMax:")
        print(f"   Input shape: {current.shape}")
        log_softmax_output = current - np.log(np.sum(np.exp(current), axis=1, keepdims=True))
        print(f"   Output shape: {log_softmax_output.shape}")
        print(f"   Sample output (first image): {log_softmax_output[0][:5]}")
        
        print(f"\nâœ… Complete CNN Forward Pass Finished!")
        print(f"Final output shape: {log_softmax_output.shape}")
        
        return log_softmax_output

def main():
    """Main demonstration of CNN parallelism"""
    print("ğŸ§  CNN Parallelism Deep Dive: Letter Recognition System")
    print("=" * 80)
    print("Architecture: Convâ†’Poolâ†’GELU Ã— 3 â†’ Linearâ†’GELU Ã— 2 â†’ Linear â†’ LogSoftMax")
    print("Parallelism: Batch, Channel, Spatial, Filter, Pipeline strategies")
    print("=" * 80)
    
    # Create sample input batch (letters A-Z classification)
    batch_size = 16  # Process 16 images simultaneously
    input_batch = np.random.randn(batch_size, 1, 28, 28).astype(np.float32)
    
    print(f"\nğŸ“¦ Input Batch Information:")
    print(f"   Batch size: {batch_size}")
    print(f"   Image dimensions: 28Ã—28Ã—1")
    print(f"   Total input elements: {input_batch.size:,}")
    print(f"   Memory usage: {input_batch.nbytes:,} bytes ({input_batch.nbytes/1024:.1f} KB)")
    
    # Create and run complete CNN
    cnn = CompleteCNNParallelDemo()
    final_output = cnn.forward_complete_parallel(input_batch)
    
    # Summary
    print(f"\nğŸ“Š CNN Parallelism Summary:")
    print(f"   Input processed: {batch_size} images")
    print(f"   Final predictions: {final_output.shape[1]} classes (A-Z)")
    print(f"   Parallelization strategies demonstrated:")
    print(f"     âœ… Batch parallelism: Process multiple images simultaneously")
    print(f"     âœ… Channel parallelism: Process feature maps concurrently")
    print(f"     âœ… Spatial parallelism: Process image regions in parallel")
    print(f"     âœ… Filter parallelism: Apply multiple kernels simultaneously")
    print(f"     âœ… Element-wise parallelism: GELU activation vectorization")
    print(f"     âœ… Matrix parallelism: Linear layer optimizations")

if __name__ == "__main__":
    # Set up environment for optimal parallel performance
    import os
    os.environ['OMP_NUM_THREADS'] = str(multiprocessing.cpu_count())
    
    main()